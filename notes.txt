DCT_layer -> LinearDCT -> old DCT linear implementation, working
DCT_conv_layer -> same -> old DCT convolution implementation, somewhat working (?) -> nope, increases trainable params, not working

ConvDCT -> new DCT convolution implementation, working 
DCT_linear_layer -> new DCT linear implementation, not working

jupyter notebooks -> not/somewhat working (?) implementations, new linear and old conv -> no more updates from now
py files -> working implementations, old linear and new conv -> only conv


17-05

after 5 epochs
					Alexnet acc: 43.7%

AlexnetDCT acc: 39.3%				AlexnetDST acc: 36.0%
AlexnetConvDCT acc: 44.1%			AlexnetConvDST acc: 38.4%
AlexnetLinearDCT acc: 45.3%			AlexnetLinearDST  acc: 46.2%

24-05 
changed network names 

for AlexnetLinearDCT/DST, replacing the classification layer gives better acc, but not for AlexnetDCT/DST

after 5 epochs, with LinearDCT/LinearDST classification layer

AlexnetDCT acc:	23.1%				AlexnetDST acc: 20.6%
AlexnetLinearDCT acc: 52.3%			AlexnetLinearDST acc: 51.3%


AlexnetLinearDFT: loss function problem

08-06

					trainable params

AlexnetDCT: 18723					AlexnetDST: 18723
AlexnetConvDCT: 21023001			AlexnetConvDST: 21023001
AlexnetLinearDCT: 2267988			AlexnetLinearDST: 2267988

AlexnetLinearDFT: 2276190

after 5 epochs
AlexnetLinearDFT acc: 44.7% 

note to self: check out MLFlow and migrate to pytorch lightning


existing prototype scripts not usable after changing models to pytorch_lightning, maybe discard?

ignore old accuracies and params count, from now just consider model_summary and lightning_logs metrics

10-06

Conv2dDFT complex output is not compatible with maxpool2d, works for maxpool3d, but shape has '1' in last dim, 
not suitable for either complex or non-complex input into next Conv2dDFT layer, maybe squeeze the last dim(?).
or just take mean of 'y' after every Conv2dDFT(?), then no more complex output at all for both these ideas, 
but maxpool3d is not needed when taking mean

Also, taking mean at the end of forward function is probably not mathematically correct. 

alternative: take sum, same in Conv2dDFT.

sum seems to be better performing in AlexNetLinearDFT, reproducible tests needed

AlexnetConvDFT validation step error

13-06
missing return value in forward function in AlexnetConvDCT, fixed

AlexnetConvDFT validation loss nan, normalization prob, fixed, other variations seem to be ok

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers.

21-06
recursive replace layer doesn't work entirely for resnet,

DenseNetLinearDCT ok, DenseNetConvDCT doesn't train

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers. + wavelet layers

23-06
DenseNet needs to be fixed from scratch, following implementation details for CIFAR in paper. https://arxiv.org/pdf/1608.06993v5.pdf
prob: size of feature map of dense block , DenseNet.py line 170

added drop_rate=0.2

not sure if its worth doing this, DenseNetConvDCT is still not training in current state, maybe just change kernel in one block?

even changing one block doesn't work, need to look into normalization

01-07
Find a way to compare normal conv layers and kernel-parameterized conv layers, check variance (how many params needed for x% of variance), 
check which features are more prominent, visualize the filters,  (prof. vollmer ideas), find an explanantion whatever can be seen (diffs or no diffs)
maybe haar and daubechies wavelets, in addition to shannon wavelets, so both CWT and DWT

04-07
datadings for imagenet - pypi - joachim folz

fix conv2d implementation
check on lenet,mnist, alexnet, CIFAR
MLflow integration

complex layers - use cardioid activation, stella, uni berkely - no relu
complex parts, use some epsilon

complex networks forward - sum of squares

complex - custom batchnorm2d for two parts, real and complex - concatenatedBatchnorm

05-07
conv2dDCT not orthonormal, both versions, but older is more closer to orthonormal

06-07
new version now orthonormal, and somewhat orthonormal for non-square kernel_size, better than old version 
looks same for square kernel_size.

in new version, inner product of 2 vectors is closer to zero than old version for non-square kernel_size, zero for square kernel_size. 

in old version, very close to zero for square kernel_size

https://www.commsp.ee.ic.ac.uk/~tania/teaching/Maths%20for%20Signals%20and%20Systems/Lectures%208-9%202015.pdf

but old version gives more test accuracy, for now, weird!!

similar for conv2dDST, except inner product is closer to zero in old version than new, for square kernel_size, 
vice versa for non-square kernel_size

maybe just overcomplicating things with seperate filters for height and width, then merging them together, too much loss of info

14-07
CWT still not orthonormal, all the conv2d was fixed, maybe overcomplicating, but new version is the correct way,

conv2dDFT was so wrong, weighted sum works better for dft but not dct, didn't test dst, need more testing 

fixed all the other layers, was dividing by out_channels in the kernels, but N=in_channels

new conv2ds are very slow, especially Conv2dDFT

15-07
cardioid shape mismatch, works with hacky z.sum(-1), 

cReLU works combined with cMaxPool2d

test_acc in AlexnetConvDFT with cReLU and cMaxPool2d is worse than previous hacky way of y.sum(-1), no complex output from conv2dDFT

cardioid can work with normal maxpool2d, seems to be ok LeNetConvDFT

01-08

ConvDFT sum in line 225, maybe wrong

ComplexAvgPool2d and ComplexAdadptiveAvgPool2d are probably wrong

24-08

MNIST

orig - 
ConvDCT - 
ConvDST - 
ConvRealShannon - 

DCT - 
DST -  
DFT - 
RealShannon - 
shannon - 


06-12

    16.12.2022: pre-draft (outline is defined on both top- and within-chapter level, >=60% of writing is done), please share as PDF per email
    20.01.2023: first draft (all chapters written, experimental results might be pending), please share as PDF per email
    10.02.2023: second draft (experiments and writing finished), please share as PDF per email
    03.03.2023: final draft, please share as PDF per email; schedule defense
    17.03.2023: ready to submit
    31.03.2023: thesis deadline <- please let me know if this date is to be adjusted


	 1. As for the ablation study, I'd suggest you to focus on the best model for each dataset.  
	 However, I believe that extending the study to other models should be beneficial to your thesis.
    2. The theoretical part should cover the mathematical basis of how the network works. 
	A nice example of a master's thesis that might be relevant to you can be found here: https://arxiv.org/abs/1602.09046


1. All milestones completed on time;
    2. Experiments cover:
        1. Kernels: DCT, DST, DFT, Shannon (real, complex);
        2. Datasets: MNIST, CIFAR-{10,100};
        3. Architectures: LeNet, AlexNet, ResNet-{18,50}, DenseNet-{121,201};
        4. Ablation study: influence of compressed layers on the model performance (how performance changes when replacing regular layers by compressed ones);
    3. Theory must include: forward and backward pass calculations (activations, regular and compressed layers, incl. complex-valued), 
	optimization procedure;
    4. Formal requirements of the Department fulfilled.

<<<<<<< HEAD






old plan

thesis plan


To do:
1. High-level SciPy prototype (if case you need a reference implementation)
2. Purely NumPy-implemented prototype
3. Prototype based on PyTorch (make sure that things are differentiable)
4. Fully functional layer implemented in PyTorch

experimental pipeline:
1. Data loading
2. Data split (training/validation/testing)
3. Model training (with logs and snapshots)
4. Model evaluation

thesis plan for now:
1) Implement a DCT-compressed FC-layer. Evaluate it on some small datasets (e.g., CIFAR-10/100, Caltech-UCSD Birds 200, etc.) 
by using compressed layers instead of regular ones. For example, AlexNet is good for this kind of experiments, 
since the majority of the parameters (58.6M out of 60.9M) belongs to the FC-part. However, 
it's also needed to quantify the effect on other architectures (e.g., ResNet, DenseNet, etc.). 
Optionally, consider implementing the layer in the way that you could replace kernels, so you'd be able to evaluate different ones.

2) Implement a DCT-compressed Conv2d-layer. Run the same evaluation procedure, but now the difference will be that only Conv2d-layers are compressed, 
while FC-ones are kept regular. Same optional point about the kernel replacement. 
Note about the 2d-DCT: the 2d-case can be separated into two 1d-cases; in other words, 
you take the DCT along the X-axis first, then you apply the same kernel to the obtained result along the Y-axis.

3) Finally, in the chosen models, replace all Conv2d- and FC-layers with the compressed counterparts and evaluate 
the performance following the same procedure. This way, you'll come up with an in-depth evaluation of the effect of 
the proposed layer compression approach on model's performance and size.
=======
    nvcr.io_nvidia_pytorch_22.11-py3.sqsh

    $ srun -K \
  --job-name="lenet_test" \
  --gpus=1 \
  --container-mounts=/netscratch/pias:/netscratch/pias,/ds:/ds:ro,"`pwd`":"`pwd`" \
  --container-image=/netscratch/enroot/nvcr.io_nvidia_pytorch_22.11-py3.sqsh \
  --container-workdir="`pwd`" \
  --mail-type=ALL --mail-user=mir.pias01@gmail.com \
  python src/main.py
>>>>>>> e452af85cdbfa6f5ce53f46f41fd86604f640be6
