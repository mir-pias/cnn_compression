DCT_layer -> LinearDCT -> old DCT linear implementation, working
DCT_conv_layer -> same -> old DCT convolution implementation, somewhat working (?) -> nope, increases trainable params, not working

ConvDCT -> new DCT convolution implementation, working 
DCT_linear_layer -> new DCT linear implementation, not working

jupyter notebooks -> not/somewhat working (?) implementations, new linear and old conv -> no more updates from now
py files -> working implementations, old linear and new conv -> only conv


17-05

after 5 epochs
					Alexnet acc: 43.7%

AlexnetDCT acc: 39.3%				AlexnetDST acc: 36.0%
AlexnetConvDCT acc: 44.1%			AlexnetConvDST acc: 38.4%
AlexnetLinearDCT acc: 45.3%			AlexnetLinearDST  acc: 46.2%

24-05 
changed network names 

for AlexnetLinearDCT/DST, replacing the classification layer gives better acc, but not for AlexnetDCT/DST

after 5 epochs, with LinearDCT/LinearDST classification layer

AlexnetDCT acc:	23.1%				AlexnetDST acc: 20.6%
AlexnetLinearDCT acc: 52.3%			AlexnetLinearDST acc: 51.3%


AlexnetLinearDFT: loss function problem

08-06

					trainable params

AlexnetDCT: 18723					AlexnetDST: 18723
AlexnetConvDCT: 21023001			AlexnetConvDST: 21023001
AlexnetLinearDCT: 2267988			AlexnetLinearDST: 2267988

AlexnetLinearDFT: 2276190

after 5 epochs
AlexnetLinearDFT acc: 44.7% 

note to self: check out MLFlow and migrate to pytorch lightning


existing prototype scripts not usable after changing models to pytorch_lightning, maybe discard?

ignore old accuracies and params count, from now just consider model_summary and lightning_logs metrics

10-06

Conv2dDFT complex output is not compatible with maxpool2d, works for maxpool3d, but shape has '1' in last dim, 
not suitable for either complex or non-complex input into next Conv2dDFT layer, maybe squeeze the last dim(?).
or just take mean of 'y' after every Conv2dDFT(?), then no more complex output at all for both these ideas, 
but maxpool3d is not needed when taking mean

Also, taking mean at the end of forward function is probably not mathematically correct. 

alternative: take sum, same in Conv2dDFT.

sum seems to be better performing in AlexNetLinearDFT, reproducible tests needed

AlexnetConvDFT validation step error

13-06
missing return value in forward function in AlexnetConvDCT, fixed

AlexnetConvDFT validation loss nan, normalization prob, fixed, other variations seem to be ok

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers.

21-06
recursive replace layer doesn't work entirely for resnet,

DenseNetLinearDCT ok, DenseNetConvDCT doesn't train

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers. + wavelet layers

23-06
DenseNet needs to be fixed from scratch, following implementation details for CIFAR in paper. https://arxiv.org/pdf/1608.06993v5.pdf
prob: size of feature map of dense block , DenseNet.py line 170

added drop_rate=0.2

not sure if its worth doing this, DenseNetConvDCT is still not training in current state, maybe just change kernel in one block?

even changing one block doesn't work, need to look into normalization

01-07
Find a way to compare normal conv layers and kernel-parameterized conv layers, check variance (how many params needed for x% of variance), 
check which features are more prominent, visualize the filters,  (prof. vollmer ideas), find an explanantion whatever can be seen (diffs or no diffs)
maybe haar and daubechies wavelets, in addition to shannon wavelets, so both CWT and DWT
