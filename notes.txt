DCT_layer -> LinearDCT -> old DCT linear implementation, working
DCT_conv_layer -> same -> old DCT convolution implementation, somewhat working (?) -> nope, increases trainable params, not working

ConvDCT -> new DCT convolution implementation, working 
DCT_linear_layer -> new DCT linear implementation, not working

jupyter notebooks -> not/somewhat working (?) implementations, new linear and old conv -> no more updates from now
py files -> working implementations, old linear and new conv -> only conv


17-05

after 5 epochs
					Alexnet acc: 43.7%

AlexnetDCT acc: 39.3%				AlexnetDST acc: 36.0%
AlexnetConvDCT acc: 44.1%			AlexnetConvDST acc: 38.4%
AlexnetLinearDCT acc: 45.3%			AlexnetLinearDST  acc: 46.2%

24-05 
changed network names 

for AlexnetLinearDCT/DST, replacing the classification layer gives better acc, but not for AlexnetDCT/DST

after 5 epochs, with LinearDCT/LinearDST classification layer

AlexnetDCT acc:	23.1%				AlexnetDST acc: 20.6%
AlexnetLinearDCT acc: 52.3%			AlexnetLinearDST acc: 51.3%


AlexnetLinearDFT: loss function problem

08-06

					trainable params

AlexnetDCT: 18723					AlexnetDST: 18723
AlexnetConvDCT: 21023001			AlexnetConvDST: 21023001
AlexnetLinearDCT: 2267988			AlexnetLinearDST: 2267988

AlexnetLinearDFT: 2276190

after 5 epochs
AlexnetLinearDFT acc: 44.7% 

note to self: check out MLFlow and migrate to pytorch lightning


existing prototype scripts not usable after changing models to pytorch_lightning, maybe discard?

ignore old accuracies and params count, from now just consider model_summary and lightning_logs metrics

10-06

Conv2dDFT complex output is not compatible with maxpool2d, works for maxpool3d, but shape has '1' in last dim, 
not suitable for either complex or non-complex input into next Conv2dDFT layer, maybe squeeze the last dim(?).
or just take mean of 'y' after every Conv2dDFT(?), then no more complex output at all for both these ideas, 
but maxpool3d is not needed when taking mean

Also, taking mean at the end of forward function is probably not mathematically correct. 

alternative: take sum, same in Conv2dDFT.

sum seems to be better performing in AlexNetLinearDFT, reproducible tests needed

AlexnetConvDFT validation step error

13-06
missing return value in forward function in AlexnetConvDCT, fixed

AlexnetConvDFT validation loss nan, normalization prob, fixed, other variations seem to be ok

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers.

21-06
recursive replace layer doesn't work entirely for resnet,

DenseNetLinearDCT ok, DenseNetConvDCT doesn't train

to do: incorportate kernel width (self.kernel_size[1]) in all conv layers. + wavelet layers